{"cells":[{"source":"![car](car.jpg)\n\nInsurance companies invest a lot of [time and money](https://www.accenture.com/_acnmedia/pdf-84/accenture-machine-leaning-insurance.pdf) into optimizing their pricing and accurately estimating the likelihood that customers will make a claim. In many countries insurance it is a legal requirement to have car insurance in order to drive a vehicle on public roads, so the market is very large!\n\nKnowing all of this, On the Road car insurance have requested your services in building a model to predict whether a customer will make a claim on their insurance during the policy period. As they have very little expertise and infrastructure for deploying and monitoring machine learning models, they've asked you to identify the single feature that results in the best performing model, as measured by accuracy, so they can start with a simple model in production.\n\nThey have supplied you with their customer data as a csv file called `car_insurance.csv`, along with a table detailing the column names and descriptions below.","metadata":{},"id":"c3f0e974-faf8-458f-bf2a-06a469d0ea5e","cell_type":"markdown","attachments":{}},{"source":"\n\n## The dataset\n\n| Column | Description |\n|--------|-------------|\n| `id` | Unique client identifier |\n| `age` | Client's age: <br> <ul><li>`0`: 16-15</li><li>`1`: 26-39</li><li>`2`: 40-64</li><li>`3`: 65+</li></ul> |\n| `gender` | Client's gender: <br> <ul><li>`0`: Female</li><li>`1`: Male</li></ul> |\n| `driving_experience` | Years the client has been driving: <br> <ul><li>`0`: 0-9</li><li>`1`: 10-19</li><li>`2`: 20-29</li><li>`3`: 30+</li></ul> |\n| `education` | Client's level of education: <br> <ul><li>`0`: No education</li><li>`1`: High school</li><li>`2`: University</li></ul> |\n| `income` | Client's income level: <br> <ul><li>`0`: Poverty</li><li>`1`: Working class</li><li>`2`: Middle class</li><li>`3`: Upper class</li></ul> |\n| `credit_score` | Client's credit score (between zero and one) |\n| `vehicle_ownership` | Client's vehicle ownership status: <br><ul><li>`0`: Does not own their vehilce (paying off finance)</li><li>`1`: Owns their vehicle</li></ul> |\n| `vehcile_year` | Year of vehicle registration: <br><ul><li>`0`: Before 2015</li><li>`1`: 2015 or later</li></ul> |\n| `married` | Client's marital status: <br><ul><li>`0`: Not married</li><li>`1`: Married</li></ul> |\n| `children` | Client's number of children |\n| `postal_code` | Client's postal code | \n| `annual_mileage` | Number of miles driven by the client each year |\n| `vehicle_type` | Type of car: <br> <ul><li>`0`: Sedan</li><li>`1`: Sports car</li></ul> |\n| `speeding_violations` | Total number of speeding violations received by the client | \n| `duis` | Number of times the client has been caught driving under the influence of alcohol |\n| `past_accidents` | Total number of previous accidents the client has been involved in |\n| `outcome` | Whether the client made a claim on their car insurance (response variable): <br><ul><li>`0`: No claim</li><li>`1`: Made a claim</li></ul> |","metadata":{},"id":"8928ffdf-25d6-4ad9-909f-0dd8d10b9a42","cell_type":"markdown"},{"source":"# Import required modules\nimport pandas as pd\nimport numpy as np\nfrom statsmodels.formula.api import logit\n\n# Start coding!\n\n# Code for Datacamp Project: \"Modeling Car Insurance Claim Outcomes\"\n\n# Import required modules\nimport pandas as pd\nimport numpy as np\nfrom statsmodels.formula.api import logit\nimport statsmodels.api as sm\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import (\n    LogisticRegression, RidgeCV\n)\nfrom sklearn.preprocessing import (\n    StandardScaler, OrdinalEncoder, OneHotEncoder\n)\n\n# Import explore_df\ndef explore_df(df, method):\n    \"\"\"\n    Function to run describe, head, or info on df.\n\n    Parameters\n    ----------\n    df : pandas.DataFrame\n        The DataFrame to explore.\n    method : {'desc', 'head', 'info', 'all'}\n        Specify the method to use.\n        - 'desc': Display summary statistics using describe().\n        - 'head': Display the first few rows using head().\n        - 'info': Display concise information about the DataFrame using info().\n        - 'na': Display counts of NAs per column and percentage of NAs per column.\n        - 'all': Display all information from above options.\n\n    Returns\n    -------\n    None\n    \"\"\"\n    if method.lower() == \"desc\":\n        print(df.describe())\n    elif method.lower() == \"head\":\n        pd.set_option('display.max_columns', None)\n        print(df.head())\n        pd.reset_option('display.max_columns')\n    elif method.lower() == \"info\":\n        print(df.info())\n    elif method.lower() == \"na\":\n        print(f\"\\n\\n<<______NA_COUNT______>>\")\n        print(df.isna().sum())\n        print(f\"\\n\\n<<______NA_PERCENT______>>\")\n        print((df.isna().sum() / df.shape[0])*100)\n    elif method.lower() == \"all\":\n        print(\"<<______HEAD______>>\")\n        pd.set_option('display.max_columns', None)\n        print(df.head())\n        pd.reset_option('display.max_columns')\n        print(f\"\\n\\n<<______DESCRIBE______>>\")\n        print(df.describe())\n        print(f\"\\n\\n<<______INFO______>>\")\n        print(df.info())\n        print(f\"\\n\\n<<______NA_COUNT______>>\")\n        print(df.isna().sum())\n        print(f\"\\n\\n<<______NA_PERCENT______>>\")\n        print((df.isna().sum() / df.shape[0])*100)\n    else:\n        print(\"Methods: 'desc', 'head', 'info' or 'all'\")\n\n# Import the data\nci = pd.read_csv(\"car_insurance.csv\")\n\n# Explore the df\nexplore_df(ci, 'all')\n\n\n# Datacamp Tasks:\n\n# Task 1. Identify the single feature of the data that is \n# the best predictor of whether a customer will put in a\n# claim (the \"outcome\" column), excluding the \"id\" column.\n\n# Task 2. Store as a DataFrame called best_feature_df, \n# containing columns named \"best_feature\" and \"best_accuracy\" \n# with the name of the feature with the highest accuracy, \n# and the respective accuracy score.\n\n# I will demonstrate 3 approaches:\n# (1) Using statsmodels but very particularly to answer task 1\n# (2) Using statsmodels (suited to inference, not for task 1)\n# (3) Using scikit (suited for ML applications and using model on new data, not for task 1)\n\n# Minimal Data Pre-processing for Short Answer:\n# 'credit_score' (%NAs: 9.82) \n# and 'annual_mileage' (%NAs: 9.57)\n# since %NA is > 5%, dropping these rows is not ideal\n# due to lack of industry knowledge we will just drop it\n# so that models can run\nci.dropna(subset=['credit_score', 'annual_mileage'], inplace=True)\n\n\n\n# Short Answer\n# (1) Using statsmodels but very particularly to answer task 1\n\ndef answer(df, x):\n    # Fit logistic regression model\n    logit_model = logit(formula='outcome ~'+x, data=df).fit()\n\n    # Calculate accuracy\n    y_pred = (logit_model.predict(df[x]) > 0.5).astype(int)  # Convert to int\n    accuracy = (y_pred == df['outcome']).mean()\n    \n    # best feature-best accuracy dict append\n    bf_ba_dict[x] = accuracy\n    \n    # notify in console output\n    print(f\"['{x}'] had an accuracy of: {accuracy}\\n\\n\")\n    \n# define list of x vars: xs\nxs = (\n    ci.drop(columns=['outcome', 'id']) # remove y var and id\n    .columns.tolist() # make them into a list to use in for-loop\n)\n\n\n# init dict\nbf_ba_dict = {}\n\n# run single var model per dv\nfor x in xs:\n    answer(ci, x)\n\n# create answer df\nbest_feature_df = pd.DataFrame({\n    'best_feature':bf_ba_dict.keys(),\n    'best_accuracy':bf_ba_dict.values()\n}).sort_values('best_accuracy', ascending=False).head(1)\nprint(best_feature_df)\n\n\n\n\n# Long (Not) Answer (at this point just working on DataSafari pkg)\n\n# Maximum Data Pre-processing and exploration\n# pre-task data inspection (0-8)\n# inspection (0): many floats in the df don't need to be, convert to int64\nvar_fl_to_int = [\n    'vehicle_ownership', # 0 1\n    'married', # 0 1\n    'outcome' # 0 1\n]\n\nfor var in var_fl_to_int:\n    ci[var] = ci[var].astype(int)\n    print(f\"Converted ['{var}'] to {ci[var].dtype}.\\n\")\n\n\n# inspection (1): here I am focused on checking out whether categorical-like variables have logical categories\n# note: these should be categorical for efficiency\nvar_inspect1 = [\n    'age', 'gender', 'driving_experience', 'education',\n    'income', 'vehicle_ownership', 'vehicle_year', 'married',\n    'vehicle_type', 'outcome'\n]\n\nfor var in var_inspect1:\n    # print unique categories:\n    print(f\"[{var}] There are {len(ci[var].unique())} unique categories, namely: {ci[var].unique()}\\n\\n\")\n\n# result: all categories are proper!\n\n\n# make ordered categorical: var_order\nvar_order = {\n    'age':[0,1,2,3],\n    'driving_experience':['0-9y','10-19y','20-29y','30y+'],\n    'education':['none','high school','university'],\n    'income':['poverty','working class','middle class','upper class'],\n    'vehicle_year':['before 2015', 'after 2015']\n}\nfor var_name, nom_order in var_order.items():\n    ci[var_name] = pd.Categorical(\n        ci[var_name],\n        categories = nom_order,\n        ordered=True\n    )\n    print(f\"Converted ['{var_name}'] to an ordinal categorical variable, with the following order:\\n{ci[var_name].unique()}\\n\\n\")\n\n\n# ordinal encode variables that need it\n# this code is for a package I'm building\n\n# define the desired order for each variable\nvar_encode_order = {\n    'driving_experience': ['0-9y', '10-19y', '20-29y', '30y+'],\n    'education': ['none', 'high school', 'university'],\n    'income': ['poverty', 'working class', 'middle class', 'upper class'],\n    'vehicle_year': ['before 2015', 'after 2015']\n}\n\n# Encode the selected variables\nfor var, order in var_encode_order.items():\n    \n    # new column name\n    new_var = var + '_encoded'\n    \n    # OrdinalEncoder /w explicit order specification\n    ord_encoder = OrdinalEncoder(categories=[order])\n    \n    ci[new_var] = ord_encoder.fit_transform(ci[[var]])\n    original_labels = ord_encoder.categories_[0].tolist()\n    print(f\"<Encoded values of ['{var}'] stored in ['{new_var}'].>\\n\\nFollowing two columns now exist:\\n{ci[[var,new_var]][0:4]}\\n\\n\")\n\n\n# make categorical: var_cat\nvar_cat = [\n    'gender', 'vehicle_ownership',\n    'married', 'vehicle_type', 'outcome'\n]\n\nfor var in var_cat:\n    ci[var] = pd.Categorical(ci[var])\n    print(f\"Converted ['{var}'] to a nominal categorical variable:\\n{ci[var].unique()}\\n\\n\")\n\n\n\n# map 'vehicle_type': 0 sedan, 1 sports car (for modeling)\n# Define mapping for vehicle_type\nvehicle_type_mapping = {'sedan': 0, 'sports car': 1}\n\n# Apply mapping to create encoded column\nci['vehicle_type'] = ci['vehicle_type'].map(vehicle_type_mapping)\n\n# sanity check\nprint(ci['vehicle_type'].unique())\n\n\n# one-hot encode variables that need it\n# this code is for a package I'm building\nvar_hot_encode = [\n    # none\n]\n\n# create an instance of OneHotEncoder\nonehot_encoder = OneHotEncoder()\n\n# encode the selected variables\nfor var in var_hot_encode:\n    # fit and transform the data\n    encoded_values = onehot_encoder.fit_transform(ci[[var]])\n    \n    # convert the sparse matrix to a DataFrame and append to ci\n    encoded_df = pd.DataFrame(\n        encoded_values.toarray(),\n        columns=onehot_encoder.get_feature_names_out([var])\n    )\n    \n    # append the new columns to df\n    ci = pd.concat([ci, encoded_df], axis=1)\n    \n    # drop the original column\n    ci.drop(columns=[var], inplace=True)\n    \n    print(f\"One-hot encoded values of ['{var}']:\\n{encoded_df.head()}\\n\\n\")\n\n\n# inspection (2): integrity check 'credit_score' values\n# inspection (3): integrity check 'children' values\n# inspection (4): integrity check 'postal_code' values\n# inspection (5): integrity check 'annual_mileage' values\n# inspection (6): integrity check 'speeding_violations' values\n# inspection (6): integrity check 'duis' values\n# inspection (7): integrity check 'past_accidents' values\nic_vars = [\n    'credit_score', 'children', 'postal_code',\n    'annual_mileage', 'speeding_violations',\n    'duis', 'past_accidents'\n]\n\nfor var in ic_vars:\n    print(f\"['{var}']\\nMean: {ci[var].mean():.2f}\\nMedian: {ci[var].median():.2f}\\nMin: {ci[var].min():.2f}\\nMax: {ci[var].max():.2f}\\n\\n\")\n    \n# all variables have values within reason, aside from:\n# - there is an observation with 22 speeding violations\n# - there is an observation with 6 DUIs\n\n\n\n# inspection (8): treatment of NAs in\n# 'credit_score' (%NAs: 9.82) \n# and 'annual_mileage' (%NAs: 9.57)\n# since %NA is > 5%, dropping these rows is not ideal\n# due to lack of industry knowledge we will just drop it\n# so that models can run\nci.dropna(subset=['credit_score', 'annual_mileage'], inplace=True)\n\n\n\n# (2) Using statsmodels (suited for inference)\n\n# Step 1: Prepare the data\nX_a1 = ci.drop(columns=[\n    'outcome', 'id', # drop y, id\n    'driving_experience', 'education',\n    'income', 'vehicle_year' # and non-encoded\n])\ny_a1 = ci['outcome']  # Target variable\n\n# Add intercept term to X\nX_a1 = sm.add_constant(X_a1)\n\n# Step 2.1: Fit a logistic regression model with all features\nfull_logit_results_a1 = sm.Logit(y_a1, X_a1).fit()\n\n# Step 2.2: Print summary of the model\nprint(full_logit_results_a1.summary())\n\n# coeff with p < 0.05\n# gender                         0.9443\n# vehicle_ownership             -1.7807\n# married                       -0.3370\n# postal_code                  2.15e-05\n# annual_mileage              7.533e-05\n# speeding_violations            0.0627\n# past_accidents                -0.1641\n# driving_experience_encoded    -1.7275\n# vehicle_year_encoded          -1.6960\n\n# Get the p-values from the model results\np_values = full_logit_results_a1.pvalues\n\n# Get the coefficients from the model results\ncoefficients = full_logit_results_a1.params\n\n# Filter coefficients based on p-values\nsignificant_coefficients = coefficients[p_values < 0.05]\n\nprint(f\"\\nThese were the significant coefficients:\\n{significant_coefficients}\\n\\n\")\n\n# Convert coefficients to log odds\nodds_ratios = np.exp(significant_coefficients)\n\nprint(f\"\\nThese are their respective log odds:\\n{odds_ratios}\\n\\n\")\n\n# Calculate the percentage change in odds\npercentage_changes = (1 - odds_ratios) * 100\n\n# Print the results\nprint(f\"\\nPercentage Change in Odds:\\n{percentage_changes}\\n\\n\")\n\n# Conclusion:\n# gender is most influential, where each unit increase in gender (from 0 to 1, or female to male), \n# the odds of a claim decrease by 157% (percent_change), \n# with a magnitute of 2.57 (log odds)\n\n\n# (3) Using scikit (suited for ML applications and using model on new data)\n\n# Step 1: Prepare the data\nX = ci.drop(columns=[\n    'outcome', 'id', # drop y, id\n    'driving_experience', 'education',\n    'income', 'vehicle_year' # and non-encoded\n])\ny = ci['outcome']  # Target variable\n\n# Split the data into training and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=4242)\n\n# Step 2.1: Fit a logistic regression model with all features\nfull_logit = LogisticRegression()\nfull_logit.fit(X_train, y_train)\n\n# Step 2.2: Evaluate full logit model\naccuracy_full = full_logit.score(X_test, y_test)\nprint(f\"Accuracy of full model: {accuracy_full}\\n\\n\")\n\n# Step 3: Fit a ridge regression model to identify important features\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nridge_model = RidgeCV(alphas=[0.1, 1.0, 10.0])\nridge_model.fit(X_train_scaled, y_train)\n\n# Step 4: Get coefficients from ridge regression model\nridge_coefs = pd.Series(ridge_model.coef_, index=X.columns)\n\n# Step 5: Identify most important features\nmost_important_features = ridge_coefs.abs().sort_values(ascending=False).index[:5]\nprint(f\"Top 4 Features According to Ridge:\\n{most_important_features}\\n\\n\")\n\n# Step 6: Fit logistic regression model with most important features\nreduced_logit = LogisticRegression()\nreduced_logit.fit(X_train[most_important_features], y_train)\n\n# Step 7: Evaluate the final model\naccuracy_reduced = reduced_logit.score(X_test[most_important_features], y_test)\nprint(f\"Accuracy of reduced model: {accuracy_reduced}\\n\\n\")\n\n# Step 8: Interpret results of model with higher accuracy\nif accuracy_full > accuracy_reduced:\n    print(\"Full model is more accurate\")\n    print(\"Coefficients:\", full_logit.coef_)\nelif accuracy_full < accuracy_reduced:\n    print(\"Coefficients:\", reduced_logit.coef_)\nelse:\n    print(\"They are equally accurate.\")","metadata":{"executionTime":10520,"id":"bA5ajAmk7XH6","lastSuccessfullyExecutedCode":"# Import required modules\nimport pandas as pd\nimport numpy as np\nfrom statsmodels.formula.api import logit\n\n# Start coding!\n\n# Code for Datacamp Project: \"Modeling Car Insurance Claim Outcomes\"\n\n# Import required modules\nimport pandas as pd\nimport numpy as np\nfrom statsmodels.formula.api import logit\nimport statsmodels.api as sm\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import (\n    LogisticRegression, RidgeCV\n)\nfrom sklearn.preprocessing import (\n    StandardScaler, OrdinalEncoder, OneHotEncoder\n)\n\n# Import explore_df\ndef explore_df(df, method):\n    \"\"\"\n    Function to run describe, head, or info on df.\n\n    Parameters\n    ----------\n    df : pandas.DataFrame\n        The DataFrame to explore.\n    method : {'desc', 'head', 'info', 'all'}\n        Specify the method to use.\n        - 'desc': Display summary statistics using describe().\n        - 'head': Display the first few rows using head().\n        - 'info': Display concise information about the DataFrame using info().\n        - 'na': Display counts of NAs per column and percentage of NAs per column.\n        - 'all': Display all information from above options.\n\n    Returns\n    -------\n    None\n    \"\"\"\n    if method.lower() == \"desc\":\n        print(df.describe())\n    elif method.lower() == \"head\":\n        pd.set_option('display.max_columns', None)\n        print(df.head())\n        pd.reset_option('display.max_columns')\n    elif method.lower() == \"info\":\n        print(df.info())\n    elif method.lower() == \"na\":\n        print(f\"\\n\\n<<______NA_COUNT______>>\")\n        print(df.isna().sum())\n        print(f\"\\n\\n<<______NA_PERCENT______>>\")\n        print((df.isna().sum() / df.shape[0])*100)\n    elif method.lower() == \"all\":\n        print(\"<<______HEAD______>>\")\n        pd.set_option('display.max_columns', None)\n        print(df.head())\n        pd.reset_option('display.max_columns')\n        print(f\"\\n\\n<<______DESCRIBE______>>\")\n        print(df.describe())\n        print(f\"\\n\\n<<______INFO______>>\")\n        print(df.info())\n        print(f\"\\n\\n<<______NA_COUNT______>>\")\n        print(df.isna().sum())\n        print(f\"\\n\\n<<______NA_PERCENT______>>\")\n        print((df.isna().sum() / df.shape[0])*100)\n    else:\n        print(\"Methods: 'desc', 'head', 'info' or 'all'\")\n\n# Import the data\nci = pd.read_csv(\"car_insurance.csv\")\n\n# Explore the df\nexplore_df(ci, 'all')\n\n\n# Datacamp Tasks:\n\n# Task 1. Identify the single feature of the data that is \n# the best predictor of whether a customer will put in a\n# claim (the \"outcome\" column), excluding the \"id\" column.\n\n# Task 2. Store as a DataFrame called best_feature_df, \n# containing columns named \"best_feature\" and \"best_accuracy\" \n# with the name of the feature with the highest accuracy, \n# and the respective accuracy score.\n\n# I will demonstrate 3 approaches:\n# (1) Using statsmodels but very particularly to answer task 1\n# (2) Using statsmodels (suited to inference, not for task 1)\n# (3) Using scikit (suited for ML applications and using model on new data, not for task 1)\n\n# Minimal Data Pre-processing for Short Answer:\n# 'credit_score' (%NAs: 9.82) \n# and 'annual_mileage' (%NAs: 9.57)\n# since %NA is > 5%, dropping these rows is not ideal\n# due to lack of industry knowledge we will just drop it\n# so that models can run\nci.dropna(subset=['credit_score', 'annual_mileage'], inplace=True)\n\n\n\n# Short Answer\n# (1) Using statsmodels but very particularly to answer task 1\n\ndef answer(df, x):\n    # Fit logistic regression model\n    logit_model = logit(formula='outcome ~'+x, data=df).fit()\n\n    # Calculate accuracy\n    y_pred = (logit_model.predict(df[x]) > 0.5).astype(int)  # Convert to int\n    accuracy = (y_pred == df['outcome']).mean()\n    \n    # best feature-best accuracy dict append\n    bf_ba_dict[x] = accuracy\n    \n    # notify in console output\n    print(f\"['{x}'] had an accuracy of: {accuracy}\\n\\n\")\n    \n# define list of x vars: xs\nxs = (\n    ci.drop(columns=['outcome', 'id']) # remove y var and id\n    .columns.tolist() # make them into a list to use in for-loop\n)\n\n\n# init dict\nbf_ba_dict = {}\n\n# run single var model per dv\nfor x in xs:\n    answer(ci, x)\n\n# create answer df\nbest_feature_df = pd.DataFrame({\n    'best_feature':bf_ba_dict.keys(),\n    'best_accuracy':bf_ba_dict.values()\n}).sort_values('best_accuracy', ascending=False).head(1)\nprint(best_feature_df)\n\n\n\n\n# Long (Not) Answer (at this point just working on DataSafari pkg)\n\n# Maximum Data Pre-processing and exploration\n# pre-task data inspection (0-8)\n# inspection (0): many floats in the df don't need to be, convert to int64\nvar_fl_to_int = [\n    'vehicle_ownership', # 0 1\n    'married', # 0 1\n    'outcome' # 0 1\n]\n\nfor var in var_fl_to_int:\n    ci[var] = ci[var].astype(int)\n    print(f\"Converted ['{var}'] to {ci[var].dtype}.\\n\")\n\n\n# inspection (1): here I am focused on checking out whether categorical-like variables have logical categories\n# note: these should be categorical for efficiency\nvar_inspect1 = [\n    'age', 'gender', 'driving_experience', 'education',\n    'income', 'vehicle_ownership', 'vehicle_year', 'married',\n    'vehicle_type', 'outcome'\n]\n\nfor var in var_inspect1:\n    # print unique categories:\n    print(f\"[{var}] There are {len(ci[var].unique())} unique categories, namely: {ci[var].unique()}\\n\\n\")\n\n# result: all categories are proper!\n\n\n# make ordered categorical: var_order\nvar_order = {\n    'age':[0,1,2,3],\n    'driving_experience':['0-9y','10-19y','20-29y','30y+'],\n    'education':['none','high school','university'],\n    'income':['poverty','working class','middle class','upper class'],\n    'vehicle_year':['before 2015', 'after 2015']\n}\nfor var_name, nom_order in var_order.items():\n    ci[var_name] = pd.Categorical(\n        ci[var_name],\n        categories = nom_order,\n        ordered=True\n    )\n    print(f\"Converted ['{var_name}'] to an ordinal categorical variable, with the following order:\\n{ci[var_name].unique()}\\n\\n\")\n\n\n# ordinal encode variables that need it\n# this code is for a package I'm building\n\n# define the desired order for each variable\nvar_encode_order = {\n    'driving_experience': ['0-9y', '10-19y', '20-29y', '30y+'],\n    'education': ['none', 'high school', 'university'],\n    'income': ['poverty', 'working class', 'middle class', 'upper class'],\n    'vehicle_year': ['before 2015', 'after 2015']\n}\n\n# Encode the selected variables\nfor var, order in var_encode_order.items():\n    \n    # new column name\n    new_var = var + '_encoded'\n    \n    # OrdinalEncoder /w explicit order specification\n    ord_encoder = OrdinalEncoder(categories=[order])\n    \n    ci[new_var] = ord_encoder.fit_transform(ci[[var]])\n    original_labels = ord_encoder.categories_[0].tolist()\n    print(f\"<Encoded values of ['{var}'] stored in ['{new_var}'].>\\n\\nFollowing two columns now exist:\\n{ci[[var,new_var]][0:4]}\\n\\n\")\n\n\n# make categorical: var_cat\nvar_cat = [\n    'gender', 'vehicle_ownership',\n    'married', 'vehicle_type', 'outcome'\n]\n\nfor var in var_cat:\n    ci[var] = pd.Categorical(ci[var])\n    print(f\"Converted ['{var}'] to a nominal categorical variable:\\n{ci[var].unique()}\\n\\n\")\n\n\n\n# map 'vehicle_type': 0 sedan, 1 sports car (for modeling)\n# Define mapping for vehicle_type\nvehicle_type_mapping = {'sedan': 0, 'sports car': 1}\n\n# Apply mapping to create encoded column\nci['vehicle_type'] = ci['vehicle_type'].map(vehicle_type_mapping)\n\n# sanity check\nprint(ci['vehicle_type'].unique())\n\n\n# one-hot encode variables that need it\n# this code is for a package I'm building\nvar_hot_encode = [\n    # none\n]\n\n# create an instance of OneHotEncoder\nonehot_encoder = OneHotEncoder()\n\n# encode the selected variables\nfor var in var_hot_encode:\n    # fit and transform the data\n    encoded_values = onehot_encoder.fit_transform(ci[[var]])\n    \n    # convert the sparse matrix to a DataFrame and append to ci\n    encoded_df = pd.DataFrame(\n        encoded_values.toarray(),\n        columns=onehot_encoder.get_feature_names_out([var])\n    )\n    \n    # append the new columns to df\n    ci = pd.concat([ci, encoded_df], axis=1)\n    \n    # drop the original column\n    ci.drop(columns=[var], inplace=True)\n    \n    print(f\"One-hot encoded values of ['{var}']:\\n{encoded_df.head()}\\n\\n\")\n\n\n# inspection (2): integrity check 'credit_score' values\n# inspection (3): integrity check 'children' values\n# inspection (4): integrity check 'postal_code' values\n# inspection (5): integrity check 'annual_mileage' values\n# inspection (6): integrity check 'speeding_violations' values\n# inspection (6): integrity check 'duis' values\n# inspection (7): integrity check 'past_accidents' values\nic_vars = [\n    'credit_score', 'children', 'postal_code',\n    'annual_mileage', 'speeding_violations',\n    'duis', 'past_accidents'\n]\n\nfor var in ic_vars:\n    print(f\"['{var}']\\nMean: {ci[var].mean():.2f}\\nMedian: {ci[var].median():.2f}\\nMin: {ci[var].min():.2f}\\nMax: {ci[var].max():.2f}\\n\\n\")\n    \n# all variables have values within reason, aside from:\n# - there is an observation with 22 speeding violations\n# - there is an observation with 6 DUIs\n\n\n\n# inspection (8): treatment of NAs in\n# 'credit_score' (%NAs: 9.82) \n# and 'annual_mileage' (%NAs: 9.57)\n# since %NA is > 5%, dropping these rows is not ideal\n# due to lack of industry knowledge we will just drop it\n# so that models can run\nci.dropna(subset=['credit_score', 'annual_mileage'], inplace=True)\n\n\n\n# (2) Using statsmodels (suited for inference)\n\n# Step 1: Prepare the data\nX_a1 = ci.drop(columns=[\n    'outcome', 'id', # drop y, id\n    'driving_experience', 'education',\n    'income', 'vehicle_year' # and non-encoded\n])\ny_a1 = ci['outcome']  # Target variable\n\n# Add intercept term to X\nX_a1 = sm.add_constant(X_a1)\n\n# Step 2.1: Fit a logistic regression model with all features\nfull_logit_results_a1 = sm.Logit(y_a1, X_a1).fit()\n\n# Step 2.2: Print summary of the model\nprint(full_logit_results_a1.summary())\n\n# coeff with p < 0.05\n# gender                         0.9443\n# vehicle_ownership             -1.7807\n# married                       -0.3370\n# postal_code                  2.15e-05\n# annual_mileage              7.533e-05\n# speeding_violations            0.0627\n# past_accidents                -0.1641\n# driving_experience_encoded    -1.7275\n# vehicle_year_encoded          -1.6960\n\n# Get the p-values from the model results\np_values = full_logit_results_a1.pvalues\n\n# Get the coefficients from the model results\ncoefficients = full_logit_results_a1.params\n\n# Filter coefficients based on p-values\nsignificant_coefficients = coefficients[p_values < 0.05]\n\nprint(f\"\\nThese were the significant coefficients:\\n{significant_coefficients}\\n\\n\")\n\n# Convert coefficients to log odds\nodds_ratios = np.exp(significant_coefficients)\n\nprint(f\"\\nThese are their respective log odds:\\n{odds_ratios}\\n\\n\")\n\n# Calculate the percentage change in odds\npercentage_changes = (1 - odds_ratios) * 100\n\n# Print the results\nprint(f\"\\nPercentage Change in Odds:\\n{percentage_changes}\\n\\n\")\n\n# Conclusion:\n# gender is most influential, where each unit increase in gender (from 0 to 1, or female to male), \n# the odds of a claim decrease by 157% (percent_change), \n# with a magnitute of 2.57 (log odds)\n\n\n# (3) Using scikit (suited for ML applications and using model on new data)\n\n# Step 1: Prepare the data\nX = ci.drop(columns=[\n    'outcome', 'id', # drop y, id\n    'driving_experience', 'education',\n    'income', 'vehicle_year' # and non-encoded\n])\ny = ci['outcome']  # Target variable\n\n# Split the data into training and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=4242)\n\n# Step 2.1: Fit a logistic regression model with all features\nfull_logit = LogisticRegression()\nfull_logit.fit(X_train, y_train)\n\n# Step 2.2: Evaluate full logit model\naccuracy_full = full_logit.score(X_test, y_test)\nprint(f\"Accuracy of full model: {accuracy_full}\\n\\n\")\n\n# Step 3: Fit a ridge regression model to identify important features\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nridge_model = RidgeCV(alphas=[0.1, 1.0, 10.0])\nridge_model.fit(X_train_scaled, y_train)\n\n# Step 4: Get coefficients from ridge regression model\nridge_coefs = pd.Series(ridge_model.coef_, index=X.columns)\n\n# Step 5: Identify most important features\nmost_important_features = ridge_coefs.abs().sort_values(ascending=False).index[:5]\nprint(f\"Top 4 Features According to Ridge:\\n{most_important_features}\\n\\n\")\n\n# Step 6: Fit logistic regression model with most important features\nreduced_logit = LogisticRegression()\nreduced_logit.fit(X_train[most_important_features], y_train)\n\n# Step 7: Evaluate the final model\naccuracy_reduced = reduced_logit.score(X_test[most_important_features], y_test)\nprint(f\"Accuracy of reduced model: {accuracy_reduced}\\n\\n\")\n\n# Step 8: Interpret results of model with higher accuracy\nif accuracy_full > accuracy_reduced:\n    print(\"Full model is more accurate\")\n    print(\"Coefficients:\", full_logit.coef_)\nelif accuracy_full < accuracy_reduced:\n    print(\"Coefficients:\", reduced_logit.coef_)\nelse:\n    print(\"They are equally accurate.\")","executionCancelledAt":null,"lastExecutedAt":1715382956641,"lastExecutedByKernel":"8cc94f32-e022-4370-9804-ea101012ee18","lastScheduledRunId":null,"outputsMetadata":{"0":{"height":556,"type":"stream"}}},"id":"d0eb4f16-5a99-460d-a5ba-706b7ef0bbe7","cell_type":"code","execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":"<<______HEAD______>>\n       id  age  gender driving_experience    education         income  \\\n0  569520    3       0               0-9y  high school    upper class   \n1  750365    0       1               0-9y         none        poverty   \n2  199901    0       0               0-9y  high school  working class   \n3  478866    0       1               0-9y   university  working class   \n4  731664    1       1             10-19y         none  working class   \n\n   credit_score  vehicle_ownership vehicle_year  married  children  \\\n0      0.629027                1.0   after 2015      0.0       1.0   \n1      0.357757                0.0  before 2015      0.0       0.0   \n2      0.493146                1.0  before 2015      0.0       0.0   \n3      0.206013                1.0  before 2015      0.0       1.0   \n4      0.388366                1.0  before 2015      0.0       0.0   \n\n   postal_code  annual_mileage vehicle_type  speeding_violations  duis  \\\n0        10238         12000.0        sedan                    0     0   \n1        10238         16000.0        sedan                    0     0   \n2        10238         11000.0        sedan                    0     0   \n3        32765         11000.0        sedan                    0     0   \n4        32765         12000.0        sedan                    2     0   \n\n   past_accidents  outcome  \n0               0      0.0  \n1               0      1.0  \n2               0      0.0  \n3               0      0.0  \n4               1      1.0  \n\n\n<<______DESCRIBE______>>\n                  id           age  ...  past_accidents       outcome\ncount   10000.000000  10000.000000  ...    10000.000000  10000.000000\nmean   500521.906800      1.489500  ...        1.056300      0.313300\nstd    290030.768758      1.025278  ...        1.652454      0.463858\nmin       101.000000      0.000000  ...        0.000000      0.000000\n25%    249638.500000      1.000000  ...        0.000000      0.000000\n50%    501777.000000      1.000000  ...        0.000000      0.000000\n75%    753974.500000      2.000000  ...        2.000000      1.000000\nmax    999976.000000      3.000000  ...       15.000000      1.000000\n\n[8 rows x 13 columns]\n\n\n<<______INFO______>>\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 10000 entries, 0 to 9999\nData columns (total 18 columns):\n #   Column               Non-Null Count  Dtype  \n---  ------               --------------  -----  \n 0   id                   10000 non-null  int64  \n 1   age                  10000 non-null  int64  \n 2   gender               10000 non-null  int64  \n 3   driving_experience   10000 non-null  object \n 4   education            10000 non-null  object \n 5   income               10000 non-null  object \n 6   credit_score         9018 non-null   float64\n 7   vehicle_ownership    10000 non-null  float64\n 8   vehicle_year         10000 non-null  object \n 9   married              10000 non-null  float64\n 10  children             10000 non-null  float64\n 11  postal_code          10000 non-null  int64  \n 12  annual_mileage       9043 non-null   float64\n 13  vehicle_type         10000 non-null  object \n 14  speeding_violations  10000 non-null  int64  \n 15  duis                 10000 non-null  int64  \n 16  past_accidents       10000 non-null  int64  \n 17  outcome              10000 non-null  float64\ndtypes: float64(6), int64(7), object(5)\nmemory usage: 1.4+ MB\nNone\n\n\n<<______NA_COUNT______>>\nid                       0\nage                      0\ngender                   0\ndriving_experience       0\neducation                0\nincome                   0\ncredit_score           982\nvehicle_ownership        0\nvehicle_year             0\nmarried                  0\nchildren                 0\npostal_code              0\nannual_mileage         957\nvehicle_type             0\nspeeding_violations      0\nduis                     0\npast_accidents           0\noutcome                  0\ndtype: int64\n\n\n<<______NA_PERCENT______>>\nid                     0.00\nage                    0.00\ngender                 0.00\ndriving_experience     0.00\neducation              0.00\nincome                 0.00\ncredit_score           9.82\nvehicle_ownership      0.00\nvehicle_year           0.00\nmarried                0.00\nchildren               0.00\npostal_code            0.00\nannual_mileage         9.57\nvehicle_type           0.00\nspeeding_violations    0.00\nduis                   0.00\npast_accidents         0.00\noutcome                0.00\ndtype: float64\nOptimization terminated successfully.\n         Current function value: 0.512632\n         Iterations 6\n['age'] had an accuracy of: 0.7740827095349122\n\n\nOptimization terminated successfully.\n         Current function value: 0.615189\n         Iterations 5\n['gender'] had an accuracy of: 0.6887961713093631\n\n\nOptimization terminated successfully.\n         Current function value: 0.466479\n         Iterations 8\n['driving_experience'] had an accuracy of: 0.7781322861700822\n\n\nOptimization terminated successfully.\n         Current function value: 0.603304\n         Iterations 5\n['education'] had an accuracy of: 0.6887961713093631\n\n\nOptimization terminated successfully.\n         Current function value: 0.533082\n         Iterations 6\n['income'] had an accuracy of: 0.7399680942446926\n\n\nOptimization terminated successfully.\n         Current function value: 0.566906\n         Iterations 6\n['credit_score'] had an accuracy of: 0.706589765615413\n\n\nOptimization terminated successfully.\n         Current function value: 0.548159\n         Iterations 5\n['vehicle_ownership'] had an accuracy of: 0.7392318075837526\n\n\nOptimization terminated successfully.\n         Current function value: 0.572577\n         Iterations 6\n['vehicle_year'] had an accuracy of: 0.6887961713093631\n\n\nOptimization terminated successfully.\n         Current function value: 0.587413\n         Iterations 5\n['married'] had an accuracy of: 0.6887961713093631\n\n\nOptimization terminated successfully.\n         Current function value: 0.594367\n         Iterations 5\n['children'] had an accuracy of: 0.6887961713093631\n\n\nOptimization terminated successfully.\n         Current function value: 0.615691\n         Iterations 5\n['postal_code'] had an accuracy of: 0.6887961713093631\n\n\nOptimization terminated successfully.\n         Current function value: 0.602726\n         Iterations 5\n['annual_mileage'] had an accuracy of: 0.692723033501043\n\n\nOptimization terminated successfully.\n         Current function value: 0.620057\n         Iterations 5\n['vehicle_type'] had an accuracy of: 0.6887961713093631\n\n\nOptimization terminated successfully.\n         Current function value: 0.556681\n         Iterations 7\n['speeding_violations'] had an accuracy of: 0.6887961713093631\n\n\nOptimization terminated successfully.\n         Current function value: 0.596760\n         Iterations 6\n['duis'] had an accuracy of: 0.6887961713093631\n\n\nOptimization terminated successfully.\n         Current function value: 0.546366\n         Iterations 7\n['past_accidents'] had an accuracy of: 0.6887961713093631\n\n\n         best_feature  best_accuracy\n2  driving_experience       0.778132\nConverted ['vehicle_ownership'] to int64.\n\nConverted ['married'] to int64.\n\nConverted ['outcome'] to int64.\n\n[age] There are 4 unique categories, namely: [3 0 1 2]\n\n\n[gender] There are 2 unique categories, namely: [0 1]\n\n\n[driving_experience] There are 4 unique categories, namely: ['0-9y' '10-19y' '20-29y' '30y+']\n\n\n[education] There are 3 unique categories, namely: ['high school' 'none' 'university']\n\n\n[income] There are 4 unique categories, namely: ['upper class' 'poverty' 'working class' 'middle class']\n\n\n[vehicle_ownership] There are 2 unique categories, namely: [1 0]\n\n\n[vehicle_year] There are 2 unique categories, namely: ['after 2015' 'before 2015']\n\n\n[married] There are 2 unique categories, namely: [0 1]\n\n\n[vehicle_type] There are 2 unique categories, namely: ['sedan' 'sports car']\n\n\n[outcome] There are 2 unique categories, namely: [0 1]\n\n\nConverted ['age'] to an ordinal categorical variable, with the following order:\n[3, 0, 1, 2]\nCategories (4, int64): [0 < 1 < 2 < 3]\n\n\nConverted ['driving_experience'] to an ordinal categorical variable, with the following order:\n['0-9y', '10-19y', '20-29y', '30y+']\nCategories (4, object): ['0-9y' < '10-19y' < '20-29y' < '30y+']\n\n\nConverted ['education'] to an ordinal categorical variable, with the following order:\n['high school', 'none', 'university']\nCategories (3, object): ['none' < 'high school' < 'university']\n\n\nConverted ['income'] to an ordinal categorical variable, with the following order:\n['upper class', 'poverty', 'working class', 'middle class']\nCategories (4, object): ['poverty' < 'working class' < 'middle class' < 'upper class']\n\n\nConverted ['vehicle_year'] to an ordinal categorical variable, with the following order:\n['after 2015', 'before 2015']\nCategories (2, object): ['before 2015' < 'after 2015']\n\n\n<Encoded values of ['driving_experience'] stored in ['driving_experience_encoded'].>\n\nFollowing two columns now exist:\n  driving_experience  driving_experience_encoded\n0               0-9y                         0.0\n1               0-9y                         0.0\n2               0-9y                         0.0\n3               0-9y                         0.0\n\n\n<Encoded values of ['education'] stored in ['education_encoded'].>\n\nFollowing two columns now exist:\n     education  education_encoded\n0  high school                1.0\n1         none                0.0\n2  high school                1.0\n3   university                2.0\n\n\n<Encoded values of ['income'] stored in ['income_encoded'].>\n\nFollowing two columns now exist:\n          income  income_encoded\n0    upper class             3.0\n1        poverty             0.0\n2  working class             1.0\n3  working class             1.0\n\n\n<Encoded values of ['vehicle_year'] stored in ['vehicle_year_encoded'].>\n\nFollowing two columns now exist:\n  vehicle_year  vehicle_year_encoded\n0   after 2015                   1.0\n1  before 2015                   0.0\n2  before 2015                   0.0\n3  before 2015                   0.0\n\n\nConverted ['gender'] to a nominal categorical variable:\n[0, 1]\nCategories (2, int64): [0, 1]\n\n\nConverted ['vehicle_ownership'] to a nominal categorical variable:\n[1, 0]\nCategories (2, int64): [0, 1]\n\n\nConverted ['married'] to a nominal categorical variable:\n[0, 1]\nCategories (2, int64): [0, 1]\n\n\nConverted ['vehicle_type'] to a nominal categorical variable:\n['sedan', 'sports car']\nCategories (2, object): ['sedan', 'sports car']\n\n\nConverted ['outcome'] to a nominal categorical variable:\n[0, 1]\nCategories (2, int64): [0, 1]\n\n\n[0, 1]\nCategories (2, int64): [0, 1]\n['credit_score']\nMean: 0.52\nMedian: 0.53\nMin: 0.05\nMax: 0.96\n\n\n['children']\nMean: 0.69\nMedian: 1.00\nMin: 0.00\nMax: 1.00\n\n\n['postal_code']\nMean: 19725.57\nMedian: 10238.00\nMin: 10238.00\nMax: 92101.00\n\n\n['annual_mileage']\nMean: 11693.46\nMedian: 12000.00\nMin: 2000.00\nMax: 22000.00\n\n\n['speeding_violations']\nMean: 1.49\nMedian: 0.00\nMin: 0.00\nMax: 22.00\n\n\n['duis']\nMean: 0.24\nMedian: 0.00\nMin: 0.00\nMax: 6.00\n\n\n['past_accidents']\nMean: 1.07\nMedian: 0.00\nMin: 0.00\nMax: 15.00\n\n\nOptimization terminated successfully.\n         Current function value: 0.356735\n         Iterations 7\n                           Logit Regression Results                           \n==============================================================================\nDep. Variable:                outcome   No. Observations:                 8149\nModel:                          Logit   Df Residuals:                     8132\nMethod:                           MLE   Df Model:                           16\nDate:                Fri, 10 May 2024   Pseudo R-squ.:                  0.4247\nTime:                        23:15:55   Log-Likelihood:                -2907.0\nconverged:                       True   LL-Null:                       -5052.9\nCovariance Type:            nonrobust   LLR p-value:                     0.000\n==============================================================================================\n                                 coef    std err          z      P>|z|      [0.025      0.975]\n----------------------------------------------------------------------------------------------\nconst                          0.4596      0.258      1.779      0.075      -0.047       0.966\nage                           -0.0181      0.050     -0.362      0.718      -0.116       0.080\ngender                         0.9443      0.071     13.240      0.000       0.804       1.084\ncredit_score                   0.2706      0.363      0.746      0.456      -0.440       0.982\nvehicle_ownership             -1.7807      0.075    -23.710      0.000      -1.928      -1.633\nmarried                       -0.3370      0.078     -4.312      0.000      -0.490      -0.184\nchildren                      -0.1403      0.078     -1.792      0.073      -0.294       0.013\npostal_code                  2.15e-05   1.81e-06     11.878      0.000     1.8e-05     2.5e-05\nannual_mileage              7.533e-05   1.48e-05      5.086      0.000    4.63e-05       0.000\nvehicle_type                   0.0487      0.153      0.319      0.750      -0.251       0.348\nspeeding_violations            0.0627      0.028      2.215      0.027       0.007       0.118\nduis                           0.0553      0.086      0.641      0.522      -0.114       0.224\npast_accidents                -0.1641      0.041     -4.000      0.000      -0.244      -0.084\ndriving_experience_encoded    -1.7275      0.082    -21.123      0.000      -1.888      -1.567\neducation_encoded             -0.0026      0.055     -0.048      0.962      -0.110       0.104\nincome_encoded                -0.0422      0.055     -0.762      0.446      -0.151       0.066\nvehicle_year_encoded          -1.6960      0.090    -18.822      0.000      -1.873      -1.519\n==============================================================================================\n\nThese were the significant coefficients:\ngender                        0.944271\nvehicle_ownership            -1.780695\nmarried                      -0.337012\npostal_code                   0.000022\nannual_mileage                0.000075\nspeeding_violations           0.062745\npast_accidents               -0.164087\ndriving_experience_encoded   -1.727517\nvehicle_year_encoded         -1.695972\ndtype: float64\n\n\n\nThese are their respective log odds:\ngender                        2.570939\nvehicle_ownership             0.168521\nmarried                       0.713900\npostal_code                   1.000022\nannual_mileage                1.000075\nspeeding_violations           1.064756\npast_accidents                0.848668\ndriving_experience_encoded    0.177725\nvehicle_year_encoded          0.183421\ndtype: float64\n\n\n\nPercentage Change in Odds:\ngender                       -157.093900\nvehicle_ownership              83.147904\nmarried                        28.609966\npostal_code                    -0.002150\nannual_mileage                 -0.007533\nspeeding_violations            -6.475559\npast_accidents                 15.133159\ndriving_experience_encoded     82.227481\nvehicle_year_encoded           81.657913\ndtype: float64\n\n\nAccuracy of full model: 0.80920245398773\n\n\nTop 4 Features According to Ridge:\nIndex(['driving_experience_encoded', 'vehicle_ownership',\n       'vehicle_year_encoded', 'gender', 'postal_code'],\n      dtype='object')\n\n\nAccuracy of reduced model: 0.747239263803681\n\n\nFull model is more accurate\nCoefficients: [[-2.59645256e-01  6.53914472e-03 -3.56963301e-02 -1.03367403e-01\n  -8.05495688e-02 -7.47870141e-02  1.74796783e-05  6.15198483e-05\n  -1.15036345e-03 -3.76888425e-01 -5.89001021e-02 -2.81902931e-01\n  -2.63616981e-01 -9.76829021e-02 -2.82176645e-01 -7.07424401e-02]]\n"}]}],"metadata":{"colab":{"name":"Welcome to DataCamp Workspaces.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.8.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"vscode":{"interpreter":{"hash":"aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"}},"editor":"DataCamp Workspace"},"nbformat":4,"nbformat_minor":5}